{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94ab92b",
   "metadata": {},
   "source": [
    "---\n",
    "### THEORY QUESTIONS:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab79c32",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">\n",
    "\n",
    "\n",
    "###  K-Nearest Neighbors (KNN)\n",
    "\n",
    "**1. What is K-Nearest Neighbors (KNN) and how does it work?**\n",
    "KNN is a supervised learning algorithm used for classification and regression. It predicts the output based on the ‘k’ closest data points in the training set using a distance metric (like Euclidean distance).\n",
    "\n",
    "**2. What is the difference between KNN Classification and KNN Regression?**\n",
    "\n",
    "* Classification: Predicts a class label based on the majority vote of k neighbors.\n",
    "* Regression: Predicts a continuous value by averaging the values of k neighbors.\n",
    "\n",
    "**3. What is the role of the distance metric in KNN?**\n",
    "The distance metric (e.g., Euclidean, Manhattan) determines how closeness is measured between points, directly affecting the accuracy of predictions.\n",
    "\n",
    "**4. What is the Curse of Dimensionality in KNN?**\n",
    "In high-dimensional spaces, data points become equidistant, making it hard for KNN to find meaningful neighbors. This degrades performance.\n",
    "\n",
    "**5. How can we choose the best value of K in KNN?**\n",
    "Use techniques like cross-validation to find the k that gives the best performance on the validation set.\n",
    "\n",
    "**6. What are KD Tree and Ball Tree in KNN?**\n",
    "They are data structures that speed up nearest neighbor search by organizing the training data in tree formats:\n",
    "\n",
    "* KD Tree: Good for low-dimensional data.\n",
    "* Ball Tree: Works better for high-dimensional data.\n",
    "\n",
    "**7. When should you use KD Tree vs. Ball Tree?**\n",
    "\n",
    "* Use KD Tree for data with ≤ 20 dimensions.\n",
    "* Use Ball Tree for higher dimensions or non-uniform data.\n",
    "\n",
    "**8. What are the disadvantages of KNN?**\n",
    "\n",
    "* Slow prediction time for large datasets\n",
    "* Sensitive to irrelevant features and feature scaling\n",
    "* Poor performance in high dimensions\n",
    "\n",
    "**9. How does feature scaling affect KNN?**\n",
    "Since KNN relies on distances, unscaled features can dominate the distance metric. Always apply standardization or normalization.\n",
    "\n",
    "---\n",
    "\n",
    "###  Principal Component Analysis (PCA)\n",
    "\n",
    "**10. What is PCA (Principal Component Analysis)?**\n",
    "PCA is an unsupervised dimensionality reduction technique that transforms data to a new coordinate system to reduce the number of features while retaining most variance.\n",
    "\n",
    "**11. How does PCA work?**\n",
    "\n",
    "1. Standardize the data\n",
    "2. Compute the covariance matrix\n",
    "3. Calculate eigenvectors and eigenvalues\n",
    "4. Project data onto top k eigenvectors (principal components)\n",
    "\n",
    "**12. What is the geometric intuition behind PCA?**\n",
    "PCA rotates the axes to align with the directions of maximum variance, projecting data into a space that captures the most information with fewer dimensions.\n",
    "\n",
    "**13. What are Eigenvalues and Eigenvectors in PCA?**\n",
    "\n",
    "* Eigenvectors: Directions of maximum variance (principal components).\n",
    "* Eigenvalues: Amount of variance captured by each eigenvector.\n",
    "\n",
    "**14. What is the difference between Feature Selection and Feature Extraction?**\n",
    "\n",
    "* Feature Selection: Selects a subset of existing features.\n",
    "* Feature Extraction: Creates new features (e.g., PCA) by transforming the original ones.\n",
    "\n",
    "**15. How do you decide the number of components to keep in PCA?**\n",
    "Use the explained variance ratio to choose enough components that retain 95-99% of the original variance.\n",
    "\n",
    "**16. Can PCA be used for classification?**\n",
    "Yes. PCA reduces dimensionality before applying a classification algorithm to improve speed and possibly accuracy.\n",
    "\n",
    "**17. What are the limitations of PCA?**\n",
    "\n",
    "* Linear technique – can’t capture non-linear relationships\n",
    "* Components are not always interpretable\n",
    "* Sensitive to data scaling\n",
    "\n",
    "**18. How do KNN and PCA complement each other?**\n",
    "PCA can reduce dimensionality before applying KNN, which improves KNN's performance and mitigates the curse of dimensionality.\n",
    "\n",
    "**19. How does KNN handle missing values in a dataset?**\n",
    "KNN typically doesn’t handle missing values directly. Missing values must be imputed before applying KNN.\n",
    "\n",
    "**20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?**\n",
    "\n",
    "* PCA: Unsupervised, maximizes variance.\n",
    "* LDA: Supervised, maximizes class separation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a2d143",
   "metadata": {},
   "source": [
    "---\n",
    "### PRACTICAL QUESTIONS:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb5e0f",
   "metadata": {},
   "source": [
    "###  1. Train a KNN Classifier on the Iris dataset and print model accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  2. Train a KNN Regressor on a synthetic dataset and evaluate using MSE\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  3. Train a KNN Classifier using Euclidean and Manhattan distance metrics and compare accuracy\n",
    "\n",
    "```python\n",
    "for metric in ['euclidean', 'manhattan']:\n",
    "    model = KNeighborsClassifier(n_neighbors=3, metric=metric)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{metric.capitalize()} Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Train a KNN Classifier with different values of K and visualize decision boundaries\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "cmap_bold = ['red', 'blue']\n",
    "\n",
    "for k in [1, 3, 5]:\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(cmap_bold), edgecolor='k', s=20)\n",
    "    plt.title(f\"K = {k}\")\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Apply Feature Scaling before training a KNN model and compare results with unscaled data\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Without scaling\n",
    "model_unscaled = KNeighborsClassifier(n_neighbors=3)\n",
    "model_unscaled.fit(X_train, y_train)\n",
    "print(\"Unscaled Accuracy:\", accuracy_score(y_test, model_unscaled.predict(X_test)))\n",
    "\n",
    "# With scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_scaled = KNeighborsClassifier(n_neighbors=3)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "print(\"Scaled Accuracy:\", accuracy_score(y_test, model_scaled.predict(X_test_scaled)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Train a PCA model on synthetic data and print the explained variance ratio\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X, _ = make_regression(n_samples=100, n_features=5, noise=0.1)\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA\n",
    "\n",
    "```python\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "model_pca = KNeighborsClassifier(n_neighbors=3)\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "print(\"Accuracy with PCA:\", accuracy_score(y_test, model_pca.predict(X_test_pca)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'n_neighbors': list(range(1, 21))}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=params, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best K:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Train a KNN Classifier and check the number of misclassified samples\n",
    "\n",
    "```python\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "misclassified = (y_pred != y_test).sum()\n",
    "print(\"Misclassified Samples:\", misclassified)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Train a PCA model and visualize the cumulative explained variance\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA().fit(X_train_scaled)\n",
    "cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(cum_var)+1), cum_var, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adace3b9",
   "metadata": {},
   "source": [
    "### 11. Train a KNN Classifier using different values of the `weights` parameter (`uniform` vs. `distance`) and compare accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Uniform weights\n",
    "knn_uniform = KNeighborsClassifier(weights='uniform')\n",
    "knn_uniform.fit(X_train, y_train)\n",
    "y_pred_uniform = knn_uniform.predict(X_test)\n",
    "\n",
    "# Distance weights\n",
    "knn_distance = KNeighborsClassifier(weights='distance')\n",
    "knn_distance.fit(X_train, y_train)\n",
    "y_pred_distance = knn_distance.predict(X_test)\n",
    "\n",
    "# Compare accuracy\n",
    "print(\"Uniform Weights Accuracy:\", accuracy_score(y_test, y_pred_uniform))\n",
    "print(\"Distance Weights Accuracy:\", accuracy_score(y_test, y_pred_distance))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  12. Train a KNN Regressor and analyze the effect of different K values on performance\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "errors = []\n",
    "k_values = range(1, 21)\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    errors.append(mse)\n",
    "\n",
    "plt.plot(k_values, errors, marker='o')\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"KNN Regressor Performance\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Implement KNN Imputation for handling missing values in a dataset\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, _ = load_iris(return_X_y=True)\n",
    "\n",
    "# Add some missing values\n",
    "rng = np.random.RandomState(42)\n",
    "X_missing = X.copy()\n",
    "X_missing[rng.randint(0, X.shape[0], 10), rng.randint(0, X.shape[1], 10)] = np.nan\n",
    "\n",
    "# Impute using KNN\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "X_imputed = imputer.fit_transform(X_missing)\n",
    "\n",
    "print(\"Missing values before:\", np.isnan(X_missing).sum())\n",
    "print(\"Missing values after:\", np.isnan(X_imputed).sum())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Train a PCA model and visualize the data projection onto the first two principal components\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Projection (2 Components)\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 15. Train a KNN Classifier using the `kd_tree` and `ball_tree` algorithms and compare performance\n",
    "\n",
    "```python\n",
    "for algorithm in ['kd_tree', 'ball_tree']:\n",
    "    knn = KNeighborsClassifier(algorithm=algorithm)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(f\"{algorithm} Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 16. Train a PCA model on a high-dimensional dataset and visualize the Scree plot\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, _ = make_classification(n_samples=200, n_features=30, random_state=42)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 17. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 18. Train a PCA model and analyze the effect of different numbers of components on accuracy\n",
    "\n",
    "```python\n",
    "accuracies = []\n",
    "component_range = range(1, X.shape[1] + 1)\n",
    "\n",
    "for n in component_range:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_reduced, y, random_state=42)\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train_pca, y_train_pca)\n",
    "    y_pred_pca = knn.predict(X_test_pca)\n",
    "    acc = accuracy_score(y_test_pca, y_pred_pca)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "plt.plot(component_range, accuracies, marker='o')\n",
    "plt.xlabel(\"Number of PCA Components\")\n",
    "plt.ylabel(\"KNN Accuracy\")\n",
    "plt.title(\"Effect of PCA Components on Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 19. Train a KNN Classifier with different `leaf_size` values and compare accuracy\n",
    "\n",
    "```python\n",
    "leaf_sizes = range(10, 51, 10)\n",
    "for leaf in leaf_sizes:\n",
    "    knn = KNeighborsClassifier(leaf_size=leaf)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(f\"Leaf size {leaf}: Accuracy = {accuracy_score(y_test, y_pred)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 20. Train a PCA model and visualize how data points are transformed before and after PCA\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before PCA\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "ax[0].set_title(\"Original Data\")\n",
    "ax[0].set_xlabel(\"Feature 1\")\n",
    "ax[0].set_ylabel(\"Feature 2\")\n",
    "\n",
    "# After PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "ax[1].set_title(\"After PCA (2 Components)\")\n",
    "ax[1].set_xlabel(\"PC1\")\n",
    "ax[1].set_ylabel(\"PC2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839deb80",
   "metadata": {},
   "source": [
    "###  21. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  22. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
    "errors = []\n",
    "\n",
    "for metric in metrics:\n",
    "    knn = KNeighborsRegressor(metric=metric)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    errors.append(mse)\n",
    "    print(f\"{metric.capitalize()} MSE:\", mse)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 23. Train a KNN Classifier and evaluate using ROC-AUC score\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
    "\n",
    "X_train, X_test, y_train, y_test_bin = train_test_split(X, y_bin, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_prob = knn.predict_proba(X_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test_bin, y_pred_prob, multi_class='ovr')\n",
    "print(\"ROC AUC Score (multi-class OVR):\", roc_auc)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 24. Train a PCA model and visualize the variance captured by each principal component\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = load_wine(return_X_y=True)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"Variance Captured by PCA Components\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 25. Train a KNN Classifier and perform feature selection before training\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select top 5 features\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, random_state=42)\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 26. Train a PCA model and visualize the data reconstruction error after reducing dimensions\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, _ = load_wine(return_X_y=True)\n",
    "pca = PCA(n_components=5)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reconstructed = pca.inverse_transform(X_reduced)\n",
    "\n",
    "reconstruction_error = mean_squared_error(X, X_reconstructed)\n",
    "print(\"Reconstruction Error (MSE):\", reconstruction_error)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 27. Train a KNN Classifier and visualize the decision boundary\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Use only 2 features for visualization\n",
    "X_vis = X[:, :2]\n",
    "y_vis = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vis, y_vis, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Create meshgrid\n",
    "x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
    "y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap='Pastel2')\n",
    "plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_vis, cmap='Dark2', edgecolor='k')\n",
    "plt.title(\"KNN Decision Boundary (2 Features)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 28. Train a PCA model and analyze the effect of different numbers of components on data variance\n",
    "\n",
    "```python\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Effect of PCA Components on Variance\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb0692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a9a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860dc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410c080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7a7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc30599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25df4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060cc412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec354c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41fd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ebbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b81d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e59347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d935e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
