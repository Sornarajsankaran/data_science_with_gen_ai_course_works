{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89644aeb",
   "metadata": {},
   "source": [
    "___\n",
    "#  ML - MODULE 8 Decision Tree ASSIGNMENT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1966e81",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 1: What is a Decision Tree, and how does it work </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It works by recursively splitting the dataset into subsets based on the feature that provides the highest information gain or the lowest impurity. Each internal node represents a test on a feature, each branch represents the outcome of the test, and each leaf node represents a class label (in classification) or a value (in regression). The goal is to create a model that predicts the value of a target variable by learning simple decision rules from the data features.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 2: What are impurity measures in Decision Trees </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Impurity measures are metrics used to determine how mixed the classes are in a node. They help in selecting the best feature to split the data. A node is \"pure\" if all data points belong to a single class. The most commonly used impurity measures are Gini Impurity and Entropy. These measures quantify how good a feature split is at separating the classes.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 3: What is the mathematical formula for Gini Impurity </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "The Gini Impurity for a node is given by:  \n",
    "Gini = 1 - Σ(p<sub>i</sub>)²  \n",
    "where p<sub>i</sub> is the proportion of samples belonging to class i in the node. The summation is over all classes.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 4: What is the mathematical formula for Entropy </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Entropy is calculated as:  \n",
    "Entropy = - Σ(p<sub>i</sub> * log₂(p<sub>i</sub>))  \n",
    "where p<sub>i</sub> is the probability of class i at a given node. The summation is over all classes.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 5: What is Information Gain, and how is it used in Decision Trees </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Information Gain is the reduction in impurity (Entropy or Gini) after a dataset is split on an attribute. It is calculated as the difference between the impurity of the parent node and the weighted impurity of the child nodes. Decision Trees use Information Gain to select the attribute that best splits the dataset at each step.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 6: What is the difference between Gini Impurity and Entropy </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Gini Impurity and Entropy both measure the purity of a node, but they differ in calculation. Gini tends to be computationally simpler and is the default in many implementations. Entropy includes a logarithmic component and has a stronger theoretical foundation in information theory. Practically, both give similar results, but Gini is slightly faster.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 7: What is the mathematical explanation behind Decision Trees </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Mathematically, a Decision Tree partitions the feature space into axis-aligned rectangles and fits a simple model (constant value) in each region. The algorithm recursively selects the feature and threshold that maximize the Information Gain or minimize impurity. At each node, the algorithm evaluates all possible splits and chooses the one that most effectively separates the classes.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 8: What is Pre-Pruning in Decision Trees </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Pre-Pruning stops the tree from growing once a certain condition is met, such as reaching a maximum depth, minimum number of samples at a node, or minimum impurity decrease. It prevents the tree from overfitting the training data by stopping early.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 9: What is Post-Pruning in Decision Trees </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Post-Pruning involves building the entire tree first and then removing branches that do not provide significant predictive power. This is typically done using a validation set to evaluate whether pruning a subtree improves model performance.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 10: What is the difference between Pre-Pruning and Post-Pruning </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Pre-Pruning halts the tree growth during construction based on specified conditions, while Post-Pruning builds a full tree and then removes unhelpful branches. Pre-Pruning is faster but may underfit, whereas Post-Pruning is more accurate but computationally heavier.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 11: What is a Decision Tree Regressor </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "A Decision Tree Regressor is a Decision Tree algorithm used for regression tasks. Instead of class labels, it predicts continuous numeric values by splitting data to minimize variance or mean squared error at each node.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 12: What are the advantages and disadvantages of Decision Trees </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Advantages:  \n",
    "- Easy to understand and interpret  \n",
    "- Requires little data preprocessing  \n",
    "- Handles both numerical and categorical data  \n",
    "- Non-parametric and can model nonlinear relationships  \n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "* Prone to overfitting\n",
    "* Unstable to small variations in data\n",
    "* Biased towards features with more levels\n",
    "* Greedy splitting may not lead to global optimum\n",
    "\n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 13: How does a Decision Tree handle missing values </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Some Decision Tree implementations handle missing values by using surrogate splits, which are backup rules that use other features to make the split when the main feature's value is missing. Others use imputation techniques or skip samples with missing values.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 14: How does a Decision Tree handle categorical features </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Decision Trees can handle categorical features by evaluating splits based on category groupings. For example, for a feature with values A, B, and C, the tree may split as A vs (B, C). Some implementations automatically handle categorical features without one-hot encoding.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 15: What are some real-world applications of Decision Trees? </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "- Medical diagnosis: Identifying diseases based on symptoms  \n",
    "- Credit scoring: Evaluating loan applications  \n",
    "- Fraud detection: Classifying transactions as fraudulent or not  \n",
    "- Marketing: Predicting customer churn or preferences  \n",
    "- Manufacturing: Quality control and defect detection  \n",
    "</div>  \n",
    "<hr>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d4d51",
   "metadata": {},
   "source": [
    "---\n",
    "### PRACTICAL: \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ec249",
   "metadata": {},
   "source": [
    "### 16. Train a Decision Tree Classifier on the Iris dataset and print accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 17. Train using Gini Impurity and print feature importances\n",
    "\n",
    "```python\n",
    "model = DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Feature Importances:\", model.feature_importances_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 18. Train using Entropy and print accuracy\n",
    "\n",
    "```python\n",
    "model = DecisionTreeClassifier(criterion='entropy')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  19. Train Decision Tree Regressor on housing dataset and evaluate using MSE\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 20 .Visualize Decision Tree using graphviz\n",
    "\n",
    "```python\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "dot_data = export_graphviz(model, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"iris_tree\")\n",
    "graph.view()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 21. Max depth = 3 vs full tree\n",
    "\n",
    "```python\n",
    "model_depth3 = DecisionTreeClassifier(max_depth=3)\n",
    "model_full = DecisionTreeClassifier()\n",
    "\n",
    "model_depth3.fit(X_train, y_train)\n",
    "model_full.fit(X_train, y_train)\n",
    "\n",
    "acc_3 = accuracy_score(y_test, model_depth3.predict(X_test))\n",
    "acc_full = accuracy_score(y_test, model_full.predict(X_test))\n",
    "\n",
    "print(\"Accuracy with max_depth=3:\", acc_3)\n",
    "print(\"Accuracy with full tree:\", acc_full)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  22. `min_samples_split=5` vs default\n",
    "\n",
    "```python\n",
    "model_custom = DecisionTreeClassifier(min_samples_split=5)\n",
    "model_default = DecisionTreeClassifier()\n",
    "\n",
    "model_custom.fit(X_train, y_train)\n",
    "model_default.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy (min_samples_split=5):\", accuracy_score(y_test, model_custom.predict(X_test)))\n",
    "print(\"Accuracy (default):\", accuracy_score(y_test, model_default.predict(X_test)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 23. Feature scaling before training vs unscaled\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_scaled = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline_unscaled = DecisionTreeClassifier()\n",
    "\n",
    "pipeline_scaled.fit(X_train, y_train)\n",
    "pipeline_unscaled.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy (with scaling):\", accuracy_score(y_test, pipeline_scaled.predict(X_test)))\n",
    "print(\"Accuracy (without scaling):\", accuracy_score(y_test, pipeline_unscaled.predict(X_test)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 24. One-vs-Rest strategy for multiclass classification\n",
    "\n",
    "```python\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "ovr_model = OneVsRestClassifier(DecisionTreeClassifier())\n",
    "ovr_model.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ovr_model.predict(X_test)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 25. Display feature importance scores\n",
    "\n",
    "```python\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "for name, score in zip(iris.feature_names, model.feature_importances_):\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  26. Regressor with `max_depth=5` vs unrestricted\n",
    "\n",
    "```python\n",
    "model_depth5 = DecisionTreeRegressor(max_depth=5)\n",
    "model_unrestricted = DecisionTreeRegressor()\n",
    "\n",
    "model_depth5.fit(X_train, y_train)\n",
    "model_unrestricted.fit(X_train, y_train)\n",
    "\n",
    "print(\"MSE (depth=5):\", mean_squared_error(y_test, model_depth5.predict(X_test)))\n",
    "print(\"MSE (unrestricted):\", mean_squared_error(y_test, model_unrestricted.predict(X_test)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 27 .Cost Complexity Pruning and accuracy\n",
    "\n",
    "```python\n",
    "path = model.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"Alpha: {ccp_alpha:.5f} -> Accuracy: {accuracy_score(y_test, clf.predict(X_test))}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  28. Evaluate with Precision, Recall, F1-Score\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  29. Confusion matrix using seaborn\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 30.GridSearchCV for max\\_depth and min\\_samples\\_split\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757159e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
