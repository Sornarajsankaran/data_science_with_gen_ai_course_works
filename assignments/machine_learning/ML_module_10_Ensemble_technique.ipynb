{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3a94e7",
   "metadata": {},
   "source": [
    "___\n",
    "#  ML - MODULE 10 ENSEMBLE TECHNIQUE ASSIGNMENT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024155fc",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 1: Can we use Bagging for regression problems </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Yes, Bagging can be used for regression problems. In regression, Bagging typically involves training multiple regression models (like Decision Tree Regressors) on different subsets of the training data and then averaging their predictions to produce the final output. This helps in reducing variance and improving the model’s stability and accuracy.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 2: What is the difference between multiple model training and single model training </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Single model training involves building and relying on one model to make predictions. Multiple model training, on the other hand, combines several models to produce better performance. In ensemble methods like Bagging or Boosting, the outputs of multiple models are aggregated to reduce errors, improve accuracy, and enhance generalization.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 3: Explain the concept of feature randomness in Random Forest </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Feature randomness in Random Forest refers to the process of selecting a random subset of features at each split of a decision tree. This ensures that individual trees are more diverse and reduces the correlation between them, which in turn improves the overall generalization and performance of the ensemble.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 4: What is OOB (Out-of-Bag) Score </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "The OOB score is a performance metric used to evaluate Bagging and Random Forest models. Since Bagging uses bootstrap sampling, some data points are left out of the training set for each model. These left-out points (out-of-bag samples) can be used as a validation set to estimate the model’s accuracy without needing a separate validation dataset.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 5: How can you measure the importance of features in a Random Forest model </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Feature importance in Random Forest is measured by looking at how much each feature contributes to reducing impurity (such as Gini index or variance) across all trees in the forest. Alternatively, permutation importance can be used, which measures the decrease in model accuracy when a feature’s values are randomly shuffled.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 6: Explain the working principle of a Bagging Classifier </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "A Bagging Classifier builds multiple models (usually the same type) on different bootstrap samples of the training data. Each model makes its own prediction, and the final prediction is made through majority voting. This process reduces variance and helps avoid overfitting compared to a single classifier.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 7: How do you evaluate a Bagging Classifier’s performance </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "The performance of a Bagging Classifier can be evaluated using standard classification metrics like accuracy, precision, recall, F1-score, and confusion matrix. You can also use cross-validation or out-of-bag score to get a more robust estimate of the model’s performance.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 8: How does a Bagging Regressor work </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "A Bagging Regressor trains multiple regression models on different bootstrap samples of the training dataset. Each model predicts a numerical output, and the final prediction is made by averaging the outputs of all the models. This ensemble approach helps in reducing variance and improving prediction accuracy.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 9: What is the main advantage of ensemble techniques </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "The main advantage of ensemble techniques is improved performance. By combining the strengths of multiple models, ensemble methods often achieve higher accuracy, better generalization, and greater robustness than individual models.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 10: What is the main challenge of ensemble methods </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "The main challenge of ensemble methods is increased complexity. They are computationally expensive, harder to interpret, and more difficult to deploy compared to single models. Also, improper tuning or overfitting to training data is possible if not handled carefully.  \n",
    "</div>  \n",
    "<hr>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0591fe",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 11: Explain the key idea behind ensemble techniques </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "The key idea behind ensemble techniques is to combine the predictions of multiple models to produce a more accurate and robust result than any individual model alone. By aggregating different models' strengths and canceling out their weaknesses, ensemble methods reduce error and improve generalization.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 12: What is a Random Forest Classifier </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "A Random Forest Classifier is an ensemble learning algorithm that builds multiple decision trees using bootstrap samples and random subsets of features. Each tree makes a prediction, and the final output is decided by majority voting. It reduces overfitting and improves accuracy compared to a single decision tree.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 13: What are the main types of ensemble techniques </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "The main types of ensemble techniques are Bagging, Boosting, and Stacking. Bagging reduces variance by training models on different subsets. Boosting reduces bias by sequentially improving weak learners. Stacking combines different models and uses another model to learn how to best combine them.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 14: What is ensemble learning in machine learning </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Ensemble learning is a technique in machine learning where multiple models are trained and combined to solve a problem. Instead of relying on a single model, ensemble methods merge several models' predictions to improve accuracy, robustness, and performance.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 15: When should we avoid using ensemble methods </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Ensemble methods should be avoided when interpretability is crucial, the dataset is small, or computational resources are limited. In such cases, simpler models may perform adequately while being easier to deploy and explain.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 16: How does Bagging help in reducing overfitting </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Bagging helps reduce overfitting by training multiple models on different random subsets of the data and averaging their predictions. This reduces variance and smooths out fluctuations or noise that could lead a single model to overfit.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 17: Why is Random Forest better than a single Decision Tree </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Random Forest is better than a single Decision Tree because it aggregates predictions from multiple trees, reducing variance and the risk of overfitting. It also introduces feature randomness, making it more robust and accurate on unseen data.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 18: What is the role of bootstrap sampling in Bagging </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Bootstrap sampling involves creating multiple training datasets by sampling with replacement from the original dataset. Each model in the Bagging ensemble is trained on a different bootstrap sample, which promotes diversity among models and helps reduce variance.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 19: What are some real-world applications of ensemble techniques </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Ensemble techniques are used in many real-world applications such as fraud detection, medical diagnosis, stock market prediction, customer churn analysis, spam detection, recommendation systems, and image classification. Their robustness makes them ideal for high-stakes predictive modeling.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 20: What is the difference between Bagging and Boosting </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Bagging trains multiple models independently in parallel using different subsets of data and combines their outputs, typically to reduce variance. Boosting, on the other hand, trains models sequentially, where each model tries to correct the errors of its predecessor, aiming to reduce bias.  \n",
    "</div>  \n",
    "<hr>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31044953",
   "metadata": {},
   "source": [
    "---\n",
    "### PRACTICAL \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686da19",
   "metadata": {},
   "source": [
    "### 1. Bagging Classifier with Decision Trees & accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Bagging Regressor with Decision Trees & MSE evaluation\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "y_pred = bagging_reg.predict(X_test)\n",
    "print(f\"Bagging Regressor MSE: {mean_squared_error(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Random Forest Classifier on Breast Cancer dataset & feature importances\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = load_breast_cancer()\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(data.data, data.target)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for name, importance in zip(data.feature_names, rf_clf.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Random Forest Regressor vs Single Decision Tree performance\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "y_pred_dt = dt_reg.predict(X_test)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Decision Tree MSE: {mse_dt:.4f}\")\n",
    "print(f\"Random Forest MSE: {mse_rf:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Out-of-Bag (OOB) Score for Random Forest Classifier\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "wine = load_wine()\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf_clf.fit(wine.data, wine.target)\n",
    "\n",
    "print(f\"OOB Score: {rf_clf.oob_score_:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Bagging Classifier using SVM as base estimator & accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "bagging_svm = BaggingClassifier(base_estimator=SVC(kernel='linear'), n_estimators=20, random_state=42)\n",
    "bagging_svm.fit(X_train, y_train)\n",
    "y_pred = bagging_svm.predict(X_test)\n",
    "\n",
    "print(f\"Bagging with SVM Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Random Forest Classifier with different tree counts & accuracy comparison\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "for n_trees in [10, 50, 100, 200]:\n",
    "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(f\"Random Forest with {n_trees} trees Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Bagging Classifier with Logistic Regression base & AUC score\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "bagging_logreg = BaggingClassifier(base_estimator=LogisticRegression(max_iter=1000), n_estimators=30, random_state=42)\n",
    "bagging_logreg.fit(X_train, y_train)\n",
    "y_proba = bagging_logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Random Forest Regressor feature importance analysis\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(housing.data, housing.target)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for name, importance in zip(housing.feature_names, rf_reg.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Ensemble model using Bagging and Random Forest & compare accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy_score(y_test, y_pred_bagging):.4f}\")\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b76c10",
   "metadata": {},
   "source": [
    "### 11. Random Forest Classifier with GridSearchCV for hyperparameter tuning\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best params: {grid_search.best_params_}\")\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(f\"Accuracy with best params: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Bagging Regressor with different numbers of estimators & MSE comparison\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "for n_estimators in [10, 50, 100]:\n",
    "    bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
    "    bagging.fit(X_train, y_train)\n",
    "    y_pred = bagging.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Bagging Regressor with {n_estimators} estimators MSE: {mse:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Random Forest Classifier and analyze misclassified samples\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "misclassified = np.where(y_test != y_pred)[0]\n",
    "print(f\"Number of misclassified samples: {len(misclassified)}\")\n",
    "print(\"Indices of misclassified samples:\", misclassified)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Bagging Classifier vs Single Decision Tree Classifier comparison\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy_score(y_test, y_pred_bagging):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 15. Random Forest Classifier and confusion matrix visualization\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 16. Stacking Classifier with Decision Trees, SVM, Logistic Regression, compare accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('svm', SVC(kernel='linear', probability=True, random_state=42)),\n",
    "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
    "]\n",
    "\n",
    "stack_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "stack_clf.fit(X_train, y_train)\n",
    "y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 17. Random Forest Classifier and print top 5 most important features\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = load_breast_cancer()\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(data.data, data.target)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Top 5 Features:\")\n",
    "for i in indices[:5]:\n",
    "    print(f\"{data.feature_names[i]}: {importances[i]:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 18. Bagging Classifier evaluated with Precision, Recall, and F1-score\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "print(f\"Precision (macro): {precision_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(f\"Recall (macro): {recall_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(f\"F1 Score (macro): {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 19. Random Forest Classifier: effect of max\\_depth on accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "for depth in [None, 3, 5, 10]:\n",
    "    rf = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(f\"Max Depth: {depth}, Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 20. Bagging Regressor with DecisionTree and KNeighbors base estimators\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "for base in [DecisionTreeRegressor(), KNeighborsRegressor()]:\n",
    "    bagging = BaggingRegressor(base_estimator=base, n_estimators=50, random_state=42)\n",
    "    bagging.fit(X_train, y_train)\n",
    "    y_pred = bagging.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Bagging with {base.__class__.__name__} MSE: {mse:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 22. Bagging Classifier performance using cross-validation\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "\n",
    "scores = cross_val_score(bagging, X, y, cv=5)\n",
    "print(f\"Cross-validation accuracies: {scores}\")\n",
    "print(f\"Mean CV Accuracy: {scores.mean():.4f}\")\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### 23. Random Forest Classifier: plot Precision-Recall curve\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_scores = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "avg_precision = average_precision_score(y_test, y_scores)\n",
    "\n",
    "plt.plot(recall, precision, label=f'AP={avg_precision:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve - Random Forest')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 24. Stacking Classifier with Random Forest and Logistic Regression, compare accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
    "]\n",
    "\n",
    "stack_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "stack_clf.fit(X_train, y_train)\n",
    "y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 25. Bagging Regressor with different bootstrap sample sizes and performance comparison\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "for max_samples in [0.5, 0.7, 1.0]:\n",
    "    bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, max_samples=max_samples, random_state=42)\n",
    "    bagging.fit(X_train, y_train)\n",
    "    y_pred = bagging.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Bagging with max_samples={max_samples} MSE: {mse:.4f}\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
