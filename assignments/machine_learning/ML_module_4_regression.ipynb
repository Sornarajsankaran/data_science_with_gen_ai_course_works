{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4573fe7d-6621-47d3-a22e-05ca77a5be66",
   "metadata": {},
   "source": [
    "___\n",
    "#  ML - MODULE 4 Regression ASSIGNMENT\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719458f4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 1: What is Simple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Simple Linear Regression is a statistical method that models the relationship between a dependent variable (Y) and a single independent variable (X) by fitting a linear equation (Y = mX + c) to observed data. It aims to predict the value of Y based on X using a straight line. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 2: What are the key assumptions of Simple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> The key assumptions of Simple Linear Regression are: 1. Linearity: The relationship between X and Y is linear. 2. Independence: Observations are independent of each other. 3. Homoscedasticity: The variance of residuals (errors) is constant across all levels of X. 4. Normality: The residuals are normally distributed. 5. No multicollinearity (not applicable here as there is only one predictor). </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 3: What does the coefficient m represent in the equation Y=mX+c </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> The coefficient m represents the slope of the regression line. It indicates the amount of change in the dependent variable Y for a one-unit increase in the independent variable X. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 4: What does the intercept c represent in the equation Y=mX+c </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> The intercept c represents the value of the dependent variable Y when the independent variable X is zero. It is the point where the regression line crosses the Y-axis. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 5: How do we calculate the slope m in Simple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> The slope m is calculated using the formula: m = Σ((Xᵢ - X̄)(Yᵢ - Ȳ)) / Σ((Xᵢ - X̄)²) where Xᵢ and Yᵢ are the individual sample points, and X̄ and Ȳ are the means of X and Y respectively. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 6: What is the purpose of the least squares method in Simple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> The least squares method aims to find the best-fitting line by minimizing the sum of the squared differences (residuals) between the observed values and the predicted values of Y. This ensures the smallest overall error in predictions. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 7: How is the coefficient of determination (R²) interpreted in Simple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> R² measures the proportion of variance in the dependent variable Y that is explained by the independent variable X. It ranges from 0 to 1, where a higher R² indicates a better fit of the model to the data. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 8: What is Multiple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship between one dependent variable and two or more independent variables by fitting a linear equation to the data. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 9: What is the main difference between Simple and Multiple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> The main difference is the number of independent variables: Simple Linear Regression has one independent variable, while Multiple Linear Regression has two or more independent variables. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 10: What are the key assumptions of Multiple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> The key assumptions of Multiple Linear Regression include: 1. Linearity: The relationship between each independent variable and the dependent variable is linear. 2. Independence: Observations are independent. 3. Homoscedasticity: Constant variance of residuals. 4. Normality: Residuals are normally distributed. 5. No multicollinearity: Independent variables are not highly correlated with each other. 6. No autocorrelation: Residuals are not correlated with each other, especially in time series data. </div> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cf453a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 11: What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Heteroscedasticity occurs when the variance of the residuals (errors) is not constant across all levels of the independent variables. It violates the homoscedasticity assumption. This can lead to inefficient estimates and biased standard errors, causing unreliable hypothesis tests and confidence intervals in the regression model. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 12: How can you improve a Multiple Linear Regression model with high multicollinearity </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> To improve a model with high multicollinearity, you can: - Remove or combine highly correlated predictors - Use dimensionality reduction techniques like Principal Component Analysis (PCA) - Regularize the model with methods like Ridge or Lasso regression - Increase sample size if possible - Center or standardize variables to reduce correlation effects </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 13: What are some common techniques for transforming categorical variables for use in regression models </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Common techniques include: - One-hot encoding: Creating binary columns for each category - Label encoding: Assigning integer values to categories (less common for regression) - Target encoding: Replacing categories with mean target values - Dummy variable trap avoidance: Removing one category to prevent multicollinearity </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 14: What is the role of interaction terms in Multiple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Interaction terms model the combined effect of two or more independent variables on the dependent variable, capturing situations where the effect of one predictor depends on the level of another. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 15: How can the interpretation of intercept differ between Simple and Multiple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> In Simple Linear Regression, the intercept is the expected value of Y when X is zero. In Multiple Linear Regression, the intercept is the expected value of Y when all independent variables are zero, which may not always be meaningful depending on the data context. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 16: What is the significance of the slope in regression analysis, and how does it affect predictions </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> The slope quantifies the change in the dependent variable for a one-unit increase in the independent variable, holding others constant (in multiple regression). It directly affects predictions by determining the direction and magnitude of influence of predictors on the outcome. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 17: What are the limitations of using R² as a sole measure of model performance </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> R² does not indicate whether the model is appropriate, whether predictors are significant, or if assumptions are met. It always increases with more predictors, potentially leading to overfitting. It also doesn't measure prediction accuracy on new data. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 18: How would you interpret a large standard error for a regression coefficient </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> A large standard error suggests that the estimate of the regression coefficient is imprecise and may not be statistically significant. It indicates high variability in the coefficient across different samples. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 19: What is polynomial regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Polynomial regression models the relationship between the independent variable(s) and the dependent variable as an nth degree polynomial, allowing for curvature in the relationship rather than a straight line. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 20: When is polynomial regression used </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> It is used when the relationship between the dependent and independent variable(s) is nonlinear and cannot be adequately captured by a straight line. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 21: How does the intercept in a regression model provide context for the relationship between variables </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> The intercept gives the expected value of the dependent variable when all independent variables are zero, providing a baseline reference point for the model. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 22: How can heteroscedasticity be identified in residual plots, and why is it important to address it </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Heteroscedasticity appears as a funnel or pattern where residuals spread increases or decreases with predicted values. Addressing it is important because it violates regression assumptions and can lead to inefficient estimates and incorrect inference. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 23: What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R² </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> It suggests the model has many predictors that do not improve the model sufficiently. Adjusted R² penalizes unnecessary predictors, so a low adjusted R² indicates potential overfitting. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 24: Why is it important to scale variables in Multiple Linear Regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Scaling helps standardize variables, improving numerical stability, interpretability, and convergence of optimization algorithms, especially important when predictors have different units or scales. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 25: How does polynomial regression differ from linear regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Polynomial regression fits a nonlinear curve by including polynomial terms (e.g., X², X³), whereas linear regression fits a straight line with only linear terms. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 26: What is the general equation for polynomial regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βₙXⁿ + ε, where n is the degree of the polynomial. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 27: Can polynomial regression be applied to multiple variables </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Yes, polynomial regression can be extended to multiple variables by including polynomial terms and interaction terms of the independent variables. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 28: What are the limitations of polynomial regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Limitations include: - Overfitting with high-degree polynomials - Increased model complexity and interpretability difficulty - Sensitive to outliers - Extrapolation beyond the data range can be unreliable </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 29: What methods can be used to evaluate model fit when selecting the degree of a polynomial </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Cross-validation, adjusted R², Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and visual inspection of residual plots can help select the appropriate polynomial degree. </div> <hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 30: Why is visualization important in polynomial regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Visualization helps understand the shape of the relationship, detect overfitting or underfitting, and communicate model behavior effectively. </div> <hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
