{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682f435a",
   "metadata": {},
   "source": [
    "---\n",
    "### THEORY QUESTIONS:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf84670",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">\n",
    "\n",
    "**1. What is unsupervised learning in the context of machine learning?**\n",
    "Unsupervised learning is a type of machine learning where the model learns patterns from data without labeled outputs. It tries to find the inherent structure, clusters, or features in the input data without guidance.\n",
    "\n",
    "---\n",
    "\n",
    "**2. How does K-Means clustering algorithm work?**\n",
    "K-Means partitions data into K clusters by:\n",
    "\n",
    "* Initializing K centroids randomly.\n",
    "* Assigning each data point to the nearest centroid.\n",
    "* Updating centroids by calculating the mean of assigned points.\n",
    "* Repeating assignment and update until convergence (centroids no longer change).\n",
    "\n",
    "---\n",
    "\n",
    "**3. Explain the concept of a dendrogram in hierarchical clustering.**\n",
    "A dendrogram is a tree-like diagram that shows the arrangement of clusters formed by hierarchical clustering, illustrating the order and distance at which clusters are merged or split.\n",
    "\n",
    "---\n",
    "\n",
    "**4. What is the main difference between K-Means and Hierarchical Clustering?**\n",
    "K-Means partitions data into a fixed number of clusters based on centroid distance, while hierarchical clustering builds a tree of clusters either by merging (agglomerative) or splitting (divisive) without specifying the number of clusters upfront.\n",
    "\n",
    "---\n",
    "\n",
    "**5. What are the advantages of DBSCAN over K-Means?**\n",
    "\n",
    "* DBSCAN can find clusters of arbitrary shapes.\n",
    "* It identifies noise points (outliers).\n",
    "* It does not require specifying the number of clusters in advance.\n",
    "\n",
    "---\n",
    "\n",
    "**6. When would you use Silhouette Score in clustering?**\n",
    "Use Silhouette Score to evaluate how well clusters are separated and how cohesive they are. It helps determine the quality of clustering and can be used to choose the optimal number of clusters.\n",
    "\n",
    "---\n",
    "\n",
    "**7. What are the limitations of Hierarchical Clustering?**\n",
    "\n",
    "* Computationally expensive for large datasets.\n",
    "* Sensitive to noise and outliers.\n",
    "* Cannot easily undo merges or splits (no reassignments).\n",
    "* Choice of linkage method can greatly affect results.\n",
    "\n",
    "---\n",
    "\n",
    "**8. Why is feature scaling important in clustering algorithms like K-Means?**\n",
    "Because K-Means uses distance metrics (like Euclidean distance), features with larger scales can dominate the distance calculation, leading to biased clusters. Scaling ensures all features contribute equally.\n",
    "\n",
    "---\n",
    "\n",
    "**9. How does DBSCAN identify noise points?**\n",
    "Points that do not belong to any cluster (i.e., points with fewer than the minimum required neighbors within a specified radius) are labeled as noise.\n",
    "\n",
    "---\n",
    "\n",
    "**10. Define inertia in the context of K-Means.**\n",
    "Inertia is the sum of squared distances between each point and its assigned cluster centroid. It measures how internally coherent clusters areâ€”the lower the inertia, the tighter the clusters.\n",
    "\n",
    "---\n",
    "\n",
    "**11. What is the elbow method in K-Means clustering?**\n",
    "It is a technique to determine the optimal number of clusters by plotting inertia vs. number of clusters and looking for the \"elbow\" point where inertia decrease slows down significantly.\n",
    "\n",
    "---\n",
    "\n",
    "**12. Describe the concept of \"density\" in DBSCAN.**\n",
    "Density refers to the number of points within a specified radius (epsilon) around a point. A point is a core point if its neighborhood contains at least a minimum number of points (minPts).\n",
    "\n",
    "---\n",
    "\n",
    "**13. Can hierarchical clustering be used on categorical data?**\n",
    "Yes, but it requires a suitable distance metric for categorical data (e.g., Hamming distance). Direct application of Euclidean distance is not appropriate.\n",
    "\n",
    "---\n",
    "\n",
    "**14. What does a negative Silhouette Score indicate?**\n",
    "It indicates that data points might be assigned to the wrong clusters, as they are closer to neighboring clusters than their own.\n",
    "\n",
    "---\n",
    "\n",
    "**15. Explain the term \"linkage criteria\" in hierarchical clustering.**\n",
    "Linkage criteria define how the distance between two clusters is computed, such as:\n",
    "\n",
    "* Single linkage (minimum distance between points)\n",
    "* Complete linkage (maximum distance)\n",
    "* Average linkage (average distance)\n",
    "\n",
    "---\n",
    "\n",
    "**16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?**\n",
    "Because K-Means assumes clusters are spherical and equally sized, it struggles with clusters of different sizes, densities, or non-globular shapes, leading to poor assignments.\n",
    "\n",
    "---\n",
    "\n",
    "**17. What are the core parameters in DBSCAN, and how do they influence clustering?**\n",
    "\n",
    "* **Epsilon (eps):** Radius around a point to search for neighbors.\n",
    "* **minPts:** Minimum number of points required to form a dense region (cluster).\n",
    "  These parameters control cluster size and noise detection.\n",
    "\n",
    "---\n",
    "\n",
    "**18. How does K-Means++ improve upon standard K-Means initialization?**\n",
    "K-Means++ initializes centroids to be spread out by choosing initial centers probabilistically, reducing the chance of poor clustering and speeding up convergence.\n",
    "\n",
    "---\n",
    "\n",
    "**19. What is agglomerative clustering?**\n",
    "A bottom-up hierarchical clustering method where each point starts as its own cluster, and clusters are merged step-by-step based on linkage criteria until one cluster or a desired number is reached.\n",
    "\n",
    "---\n",
    "\n",
    "**20. What makes Silhouette Score a better metric than just inertia for model evaluation?**\n",
    "Silhouette Score considers both cohesion (within-cluster similarity) and separation (between-cluster differences), providing a normalized measure of clustering quality, while inertia only measures compactness without considering cluster separation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea619bd",
   "metadata": {},
   "source": [
    "---\n",
    "### PRACTICAL QUESTIONS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74945bfe",
   "metadata": {},
   "source": [
    "### 1. Generate synthetic data with 4 centers using make\\_blobs and apply K-Means clustering. Visualize using a scatter plot\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=4, random_state=42)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42).fit(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', s=30)\n",
    "plt.title('KMeans Clustering on 4-center Blobs')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "iris = load_iris()\n",
    "agglo = AgglomerativeClustering(n_clusters=3).fit(iris.data)\n",
    "print(\"First 10 labels:\", agglo.labels_[:10])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Generate synthetic data using make\\_moons and apply DBSCAN. Highlight outliers in the plot\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5).fit(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=dbscan.labels_, cmap='plasma', s=30)\n",
    "outliers = dbscan.labels_ == -1\n",
    "plt.scatter(X[outliers, 0], X[outliers, 1], c='red', s=50, label='Outliers')\n",
    "plt.title('DBSCAN on Moons with Outliers Highlighted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "wine = load_wine()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(wine.data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42).fit(X_scaled)\n",
    "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
    "print(\"Cluster sizes:\", dict(zip(unique, counts)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Use make\\_circles to generate synthetic data and cluster it using DBSCAN. Plot the result\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_circles(n_samples=400, noise=0.05, factor=0.5, random_state=42)\n",
    "dbscan = DBSCAN(eps=0.15, min_samples=5).fit(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=dbscan.labels_, cmap='coolwarm', s=30)\n",
    "plt.title('DBSCAN Clustering on Circles')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(bc.data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42).fit(X_scaled)\n",
    "print(\"Cluster centroids:\\n\", kmeans.cluster_centers_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Generate synthetic data using make\\_blobs with varying cluster standard deviations and cluster with DBSCAN\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=10).fit(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=dbscan.labels_, cmap='Set1', s=30)\n",
    "plt.title('DBSCAN on Blobs with Varying Std Dev')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(digits.data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=42).fit(X_2d)\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=kmeans.labels_, cmap='tab10', s=30)\n",
    "plt.title('KMeans Clusters on Digits PCA 2D')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Create synthetic data using make\\_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=400, centers=4, random_state=42)\n",
    "scores = []\n",
    "ks = range(2, 6)\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "    score = silhouette_score(X, kmeans.labels_)\n",
    "    scores.append(score)\n",
    "\n",
    "plt.bar(ks, scores, color='skyblue')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for KMeans')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "linked = linkage(iris.data, method='average')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "dendrogram(linked, labels=iris.target, leaf_rotation=90)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Average Linkage) - Iris')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Generate synthetic data with overlapping clusters using make\\_blobs, then apply K-Means and visualize with decision boundaries\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=42)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "cmap = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "plt.contourf(xx, yy, Z, cmap=cmap, alpha=0.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, edgecolor='k', s=30)\n",
    "plt.title('KMeans with Decision Boundaries on Overlapping Blobs')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(digits.data)\n",
    "\n",
    "dbscan = DBSCAN(eps=3, min_samples=5).fit(X_tsne)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan.labels_, cmap='tab20', s=30)\n",
    "plt.title('DBSCAN on Digits t-SNE Reduced Data')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Generate synthetic data using make\\_blobs and apply Agglomerative Clustering with complete linkage. Plot the result\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
    "agglo = AgglomerativeClustering(n_clusters=4, linkage='complete').fit(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=agglo.labels_, cmap='Set2', s=30)\n",
    "plt.title('Agglomerative Clustering with Complete Linkage on Blobs')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(bc.data)\n",
    "\n",
    "inertia = []\n",
    "ks = range(2, 7)\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(ks, inertia, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('KMeans Inertia on Breast Cancer Dataset')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 15. Generate synthetic concentric circles using make\\_circles and cluster using Agglomerative Clustering with single linkage\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_circles(n_samples=400, noise=0.05, factor=0.5, random_state=42)\n",
    "agglo = AgglomerativeClustering(n_clusters=2, linkage='single').fit(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=agglo.labels, cmap='cool', s=30)\n",
    "plt.title('Agglomerative Clustering with Single Linkage on Circles')\n",
    "plt.show()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37431464",
   "metadata": {},
   "source": [
    "### 16. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "wine = load_wine()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(wine.data)\n",
    "\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=5).fit(X_scaled)\n",
    "labels = dbscan.labels_\n",
    "\n",
    "# Count clusters ignoring noise (-1 label)\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(\"Number of clusters (excluding noise):\", n_clusters)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 17. Generate synthetic data with make\\_blobs and apply KMeans. Then plot the cluster centers on top of the data points\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X, _ = make_blobs(n_samples=400, centers=4, random_state=42)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42).fit(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', s=30, alpha=0.6)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "            c='red', marker='X', s=200, label='Centers')\n",
    "plt.title(\"KMeans with Cluster Centers\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 18. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "iris = load_iris()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(iris.data)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled)\n",
    "labels = dbscan.labels_\n",
    "\n",
    "n_noise = list(labels).count(-1)\n",
    "print(\"Number of samples identified as noise:\", n_noise)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 19. Generate synthetic non-linearly separable data using make\\_moons, apply K-Means, and visualize the clustering result\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42).fit(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='coolwarm', s=30)\n",
    "plt.title(\"KMeans Clustering on Non-linearly Separable Moons Data\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 20. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "\n",
    "digits = load_digits()\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(digits.data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=42).fit(X_pca)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], \n",
    "                     c=kmeans.labels_, cmap='tab10', s=30)\n",
    "ax.set_title(\"3D KMeans Clustering on PCA-reduced Digits\")\n",
    "plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba74f3",
   "metadata": {},
   "source": [
    "### 21. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette\\_score to evaluate the clustering\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=5, random_state=42)\n",
    "kmeans = KMeans(n_clusters=5, random_state=42).fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "score = silhouette_score(X, labels)\n",
    "print(\"Silhouette Score for KMeans clustering:\", score)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 22. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=2)\n",
    "labels = agglo.fit_predict(X_pca)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='plasma', s=30)\n",
    "plt.title('Agglomerative Clustering on Breast Cancer (PCA-reduced)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 23. Generate noisy circular data using make\\_circles and visualize clustering results from KMeans and DBSCAN side-by-side\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42).fit(X)\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5).fit(X)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='coolwarm', s=30)\n",
    "axs[0].set_title('KMeans Clustering')\n",
    "\n",
    "axs[1].scatter(X[:, 0], X[:, 1], c=dbscan.labels_, cmap='coolwarm', s=30)\n",
    "axs[1].set_title('DBSCAN Clustering')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 24. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "silhouette_vals = silhouette_samples(X, labels)\n",
    "\n",
    "plt.bar(range(len(X)), silhouette_vals, color='skyblue')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.title('Silhouette Coefficient per sample after KMeans clustering')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 25. Generate synthetic data using make\\_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=400, centers=4, random_state=42)\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
    "labels = agglo.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=30)\n",
    "plt.title(\"Agglomerative Clustering with Average Linkage\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 26. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(wine.data[:, :4], columns=wine.feature_names[:4])\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42).fit(df)\n",
    "df['Cluster'] = kmeans.labels_\n",
    "\n",
    "sns.pairplot(df, hue='Cluster', palette='Set1')\n",
    "plt.suptitle('Wine Dataset Clusters (first 4 features)', y=1.02)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 27. Generate noisy blobs using make\\_blobs and use DBSCAN to identify both clusters and noise points. Print the count\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=10).fit(X)\n",
    "labels = dbscan.labels_\n",
    "\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "\n",
    "print(f\"Clusters found: {n_clusters}\")\n",
    "print(f\"Noise points identified: {n_noise}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 28. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(digits.data)\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=10)\n",
    "labels = agglo.fit_predict(X_tsne)\n",
    "\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=30)\n",
    "plt.title('Agglomerative Clustering on t-SNE reduced Digits data')\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4231086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
