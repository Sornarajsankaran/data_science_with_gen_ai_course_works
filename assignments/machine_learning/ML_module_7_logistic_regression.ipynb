{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d2bc07",
   "metadata": {},
   "source": [
    "___\n",
    "#  ML - MODULE 7 Logistic regression ASSIGNMENT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af11fea2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 1: What is Logistic Regression, and how does it differ from Linear Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Logistic Regression is a statistical method used for binary classification problems, where the outcome is categorical (typically 0 or 1). It estimates the probability that a given input belongs to a particular class using the logistic (sigmoid) function. \n",
    "\n",
    "In contrast, Linear Regression predicts continuous numeric outcomes by fitting a linear relationship between input features and the target variable.\n",
    "\n",
    "The key difference is that Logistic Regression models probabilities and outputs values between 0 and 1, suitable for classification, while Linear Regression models continuous outcomes and can produce any real number.\n",
    "\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 2: What is the mathematical equation of Logistic Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "The logistic regression model predicts the probability \\( p \\) of the positive class using:\n",
    "\n",
    "$$\n",
    "p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "Here, $\\beta_0$ is the intercept, $\\beta_i$ are the coefficients, and $x_i$ are the input features. $\\sigma(z)$ is the sigmoid function mapping any real value to the interval (0,1).\n",
    "\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 3: Why do we use the Sigmoid function in Logistic Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "The sigmoid function converts the linear combination of inputs (which can be any real number) into a probability value between 0 and 1. This probabilistic output allows Logistic Regression to model the likelihood of the input belonging to the positive class, making it suitable for classification tasks.\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 4: What is the cost function of Logistic Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "The cost function is the Negative Log-Likelihood (also known as Log Loss or Cross-Entropy Loss):\n",
    "\n",
    "$$\n",
    "J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
    "$$\n",
    "\n",
    "where $m$ is the number of samples, $y^{(i)}$ is the true label, and $\\hat{y}^{(i)}$ is the predicted probability for the positive class.\n",
    "\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 5: What is Regularization in Logistic Regression? Why is it needed </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Regularization is a technique to add a penalty term to the cost function to prevent overfitting by discouraging complex models (large coefficients). It helps improve generalization on unseen data by shrinking or eliminating less important feature coefficients.\n",
    "\n",
    "Regularization is needed when the model is too complex, especially with high-dimensional data, to avoid overfitting and improve stability.\n",
    "\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 6: Explain the difference between Lasso, Ridge, and Elastic Net regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "    \n",
    "- **Lasso (L1) Regularization:** Adds the sum of absolute values of coefficients as a penalty. It can shrink some coefficients exactly to zero, thus performing feature selection.\n",
    "\n",
    "* **Ridge (L2) Regularization:** Adds the sum of squared coefficients as a penalty. It shrinks coefficients towards zero but does not eliminate them entirely.\n",
    "\n",
    "* **Elastic Net:** Combines L1 and L2 penalties, balancing between Ridge and Lasso. It is useful when there are correlated features and you want both feature selection and coefficient shrinkage.\n",
    "\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 7: When should we use Elastic Net instead of Lasso or Ridge </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Elastic Net is preferred when the dataset has many correlated features. Lasso may arbitrarily select one feature and ignore others, while Ridge shrinks all coefficients evenly. Elastic Net combines the strengths of both, allowing group selection and better handling of multicollinearity.\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 8: What is the impact of the regularization parameter (Î») in Logistic Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "The regularization parameter \\( \\lambda \\) controls the strength of the penalty on coefficients:\n",
    "\n",
    "* Large $\\lambda$: Stronger penalty, more shrinkage, simpler model, possibly underfitting.\n",
    "* Small $\\lambda$: Weaker penalty, less shrinkage, model fits training data closely, possibly overfitting.\n",
    "\n",
    "Choosing the right $\\lambda$ balances bias and variance.\n",
    "\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 9: What are the key assumptions of Logistic Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "- The dependent variable is binary.\n",
    "- Observations are independent.\n",
    "- There is a linear relationship between the log-odds of the outcome and the independent variables.\n",
    "- No or little multicollinearity among predictors.\n",
    "- Large sample size for stable estimates.\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 10: What are some alternatives to Logistic Regression for classification tasks </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Alternatives include:\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Support Vector Machines (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Neural Networks\n",
    "- Gradient Boosting Machines (XGBoost, LightGBM)\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 11: What are Classification Evaluation Metrics </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Metrics used to evaluate classification models include:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall (Sensitivity)\n",
    "- F1-Score\n",
    "- ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "- Confusion Matrix\n",
    "- Log Loss\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 12: How does class imbalance affect Logistic Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Class imbalance can cause Logistic Regression to be biased towards the majority class, resulting in poor predictive performance for the minority class. The model might have high accuracy but low recall/precision on the minority class, thus misleading evaluation.\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 13: What is Hyperparameter Tuning in Logistic Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Hyperparameter tuning involves selecting the best values for parameters that are not learned during training, such as the regularization strength \\( \\lambda \\), type of regularization (L1, L2), and solver type, to optimize model performance on validation data.\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 14: What are different solvers in Logistic Regression? Which one should be used </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Common solvers include:\n",
    "- **liblinear:** Good for small datasets, supports L1 and L2 regularization.\n",
    "- **newton-cg:** Handles L2 regularization, suitable for multinomial loss.\n",
    "- **lbfgs:** Good for large datasets, supports L2 and multinomial.\n",
    "- **sag/saga:** Fast for large datasets; saga supports L1 and Elastic Net.\n",
    "\n",
    "Choice depends on dataset size and regularization type; for large datasets and L2, 'lbfgs' or 'saga' is preferred.\n",
    "\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 15: How is Logistic Regression extended for multiclass classification </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "It is extended via:\n",
    "    \n",
    "- **One-vs-Rest (OvR):** Train one binary classifier per class versus all others.\n",
    "- **Softmax (Multinomial Logistic Regression):** Directly models probabilities for multiple classes using the softmax function.\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 16: What are the advantages and disadvantages of Logistic Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Advantages:\n",
    "- Simple and interpretable.\n",
    "- Efficient and fast to train.\n",
    "- Outputs calibrated probabilities.\n",
    "- Works well when the relationship is linear in log-odds.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "* Cannot capture complex nonlinear relationships.\n",
    "* Sensitive to outliers and multicollinearity.\n",
    "* Performance can degrade with high-dimensional sparse data without regularization.\n",
    "\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 17: What are some use cases of Logistic Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "- Medical diagnosis (disease presence/absence)\n",
    "- Credit scoring and risk assessment\n",
    "- Spam detection\n",
    "- Customer churn prediction\n",
    "- Marketing response modeling\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 18: What is the difference between Softmax Regression and Logistic Regression </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Softmax Regression (Multinomial Logistic Regression) generalizes Logistic Regression to multiple classes by predicting a probability distribution over multiple classes using the softmax function. Logistic Regression typically refers to binary classification using the sigmoid function.\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 19: How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "- Use OvR when classes are imbalanced or when you want independent binary classifiers.\n",
    "- Use Softmax (multinomial) when classes are mutually exclusive and balanced; it models all classes jointly, often yielding better calibrated probabilities.\n",
    "</div><hr> \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 20: How do we interpret coefficients in Logistic Regression? </div> <hr> \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> \n",
    "Coefficients represent the change in the log-odds of the positive class for a one-unit increase in the predictor, holding others constant. Exponentiating a coefficient \\( \\beta_i \\) gives the odds ratio \\( e^{\\beta_i} \\), indicating how the odds change multiplicatively with a one-unit increase in the feature.\n",
    "</div><hr> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb316a2",
   "metadata": {},
   "source": [
    "---\n",
    "### PRACTICAL QUESTIONS:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c326f2",
   "metadata": {},
   "source": [
    "### **1. Load dataset, train-test split, Logistic Regression, print accuracy**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Logistic Regression with L1 regularization (Lasso)**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"L1 Regularization Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Logistic Regression with L2 regularization (Ridge), print coefficients**\n",
    "\n",
    "```python\n",
    "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"L2 Regularization Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Logistic Regression with Elastic Net Regularization**\n",
    "\n",
    "```python\n",
    "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Elastic Net Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Multiclass Logistic Regression using `multi_class='ovr'`**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print(\"Multiclass OVR Accuracy:\", accuracy_score(y, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Hyperparameter tuning using GridSearchCV**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']  # required for L1\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Stratified K-Fold Cross-Validation with Logistic Regression**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "print(\"Stratified K-Fold Accuracy Scores:\", scores)\n",
    "print(\"Average Accuracy:\", scores.mean())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Load dataset from CSV, apply Logistic Regression, evaluate accuracy**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your CSV file path and column names\n",
    "df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"CSV Dataset Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0bb7d",
   "metadata": {},
   "source": [
    "### **9. RandomizedSearchCV for Logistic Regression**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "rand_search = RandomizedSearchCV(model, param_dist, n_iter=10, scoring='accuracy', cv=5)\n",
    "rand_search.fit(X, y)\n",
    "\n",
    "print(\"Best Parameters:\", rand_search.best_params_)\n",
    "print(\"Best Accuracy:\", rand_search.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **10. One-vs-One (OvO) Multiclass Logistic Regression**\n",
    "\n",
    "```python\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "model = OneVsOneClassifier(LogisticRegression(max_iter=1000))\n",
    "model.fit(X, y)\n",
    "print(\"OvO Accuracy:\", model.score(X, y))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Confusion Matrix Visualization**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Precision, Recall, F1-Score**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **13. Imbalanced Data with Class Weights**\n",
    "\n",
    "```python\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Balanced Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **14. Titanic Dataset + Missing Values Handling**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = sns.load_dataset('titanic').dropna(subset=['embarked'])\n",
    "df = df[['age', 'fare', 'embarked', 'sex', 'survived']].dropna()\n",
    "\n",
    "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
    "df = pd.get_dummies(df, columns=['embarked'], drop_first=True)\n",
    "\n",
    "X = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X = imp.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Titanic Accuracy:\", model.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **15. Feature Scaling (Standardization)**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train_s, X_test_s, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "model_scaled = LogisticRegression(max_iter=1000)\n",
    "model_scaled.fit(X_train_s, y_train)\n",
    "\n",
    "print(\"Accuracy with Scaling:\", model_scaled.score(X_test_s, y_test))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **16. ROC-AUC Score**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, probs))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **17. Logistic Regression with C = 0.5**\n",
    "\n",
    "```python\n",
    "model = LogisticRegression(C=0.5, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Custom C Accuracy:\", model.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **18. Identify Important Features**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "importance = model.coef_[0]\n",
    "for i, coef in enumerate(importance):\n",
    "    print(f\"Feature {i}: Coefficient = {coef}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **19. Cohenâs Kappa Score**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "print(\"Cohenâs Kappa:\", cohen_kappa_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **20. Precision-Recall Curve**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
    "\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "disp.plot()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **21. Compare Solvers (liblinear, saga, lbfgs)**\n",
    "\n",
    "```python\n",
    "for solver in ['liblinear', 'saga', 'lbfgs']:\n",
    "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"{solver} Accuracy:\", model.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **22. Matthews Correlation Coefficient (MCC)**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **23. Compare Accuracy on Raw vs Standardized Data**\n",
    "\n",
    "```python\n",
    "# Raw\n",
    "model_raw = LogisticRegression(max_iter=1000)\n",
    "model_raw.fit(X_train, y_train)\n",
    "print(\"Raw Accuracy:\", model_raw.score(X_test, y_test))\n",
    "\n",
    "# Standardized\n",
    "model_scaled = LogisticRegression(max_iter=1000)\n",
    "model_scaled.fit(X_train_s, y_train)\n",
    "print(\"Standardized Accuracy:\", model_scaled.score(X_test_s, y_test))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28415c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
