{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5105e05",
   "metadata": {},
   "source": [
    "---\n",
    "###  Theory Questions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd04e63",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">\n",
    "\n",
    "\n",
    "### 1. **What is Boosting in Machine Learning?**\n",
    "\n",
    "Boosting is an **ensemble technique** that combines multiple **weak learners** (typically decision trees) to create a strong predictive model. It works by training models sequentially, where each new model tries to correct the errors made by the previous ones.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **How does Boosting differ from Bagging?**\n",
    "\n",
    "| Feature        | Boosting                      | Bagging                         |\n",
    "| -------------- | ----------------------------- | ------------------------------- |\n",
    "| Training       | Sequential                    | Parallel                        |\n",
    "| Focus          | Correcting previous errors    | Reducing variance via averaging |\n",
    "| Example Models | AdaBoost, Gradient Boosting   | Random Forest                   |\n",
    "| Overfitting    | Less prone with proper tuning | Less prone                      |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **What is the key idea behind AdaBoost?**\n",
    "\n",
    "The key idea in AdaBoost (Adaptive Boosting) is to **assign higher weights** to the data points that are misclassified by previous models, so that subsequent models focus more on those difficult cases.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Explain the working of AdaBoost with an example**\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Start with equal weights for all training samples.\n",
    "2. Train a weak learner (e.g., decision stump).\n",
    "3. Increase weights of misclassified samples.\n",
    "4. Train the next model on the updated weights.\n",
    "5. Repeat steps 2â€“4 for `T` iterations.\n",
    "6. Final prediction is a **weighted vote** of all weak learners.\n",
    "\n",
    "**Example:**\n",
    "If sample A is misclassified in the first round, its weight increases so the next learner gives it more focus. Eventually, the combined model classifies A correctly due to this increased attention.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **What is Gradient Boosting, and how is it different from AdaBoost?**\n",
    "\n",
    "Gradient Boosting builds models sequentially like AdaBoost, but instead of adjusting weights, it **fits new models to the gradient of the loss function** (i.e., it reduces prediction error using gradient descent).\n",
    "\n",
    " **Difference:**\n",
    "\n",
    "* **AdaBoost** uses exponential loss and updates weights.\n",
    "* **Gradient Boosting** uses any differentiable loss and fits residual errors directly.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **What is the loss function in Gradient Boosting?**\n",
    "\n",
    "The loss function in Gradient Boosting depends on the problem:\n",
    "\n",
    "* **Regression:** Mean Squared Error (MSE)\n",
    "* **Classification:** Log Loss (Binary Cross Entropy)\n",
    "* It uses **gradients of the loss** to train subsequent learners.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **How does XGBoost improve over traditional Gradient Boosting?**\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) offers:\n",
    "\n",
    "* **Regularization** to reduce overfitting\n",
    "* **Parallelization** of tree construction\n",
    "* **Tree pruning** using max depth and min child weight\n",
    "* **Efficient handling of missing values**\n",
    "* **Out-of-core computation** for large datasets\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **What is the difference between XGBoost and CatBoost?**\n",
    "\n",
    "| Feature   | XGBoost             | CatBoost                               |\n",
    "| --------- | ------------------- | -------------------------------------- |\n",
    "| Data Type | Needs preprocessing | Handles categorical features natively  |\n",
    "| Speed     | Very fast           | Optimized for categorical data         |\n",
    "| Accuracy  | High                | Often higher with categorical features |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **What are some real-world applications of Boosting techniques?**\n",
    "\n",
    "* **Fraud detection**\n",
    "* **Credit scoring**\n",
    "* **Customer churn prediction**\n",
    "* **Search ranking**\n",
    "* **Medical diagnosis**\n",
    "* **Click-through rate prediction**\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **How does regularization help in XGBoost?**\n",
    "\n",
    "Regularization in XGBoost:\n",
    "\n",
    "* Controls model complexity (via L1/L2 penalties)\n",
    "* Prevents overfitting\n",
    "* Helps in selecting simpler trees by penalizing large weights and complex trees\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **What are some hyperparameters to tune in Gradient Boosting models?**\n",
    "\n",
    "Key hyperparameters:\n",
    "\n",
    "* `n_estimators`: Number of boosting rounds\n",
    "* `learning_rate`: Shrinks the contribution of each tree\n",
    "* `max_depth`: Max depth of individual trees\n",
    "* `min_child_weight`: Minimum samples per leaf\n",
    "* `subsample`: Fraction of samples used per tree\n",
    "* `colsample_bytree`: Fraction of features used per tree\n",
    "* `gamma`: Minimum loss reduction for further splits\n",
    "* `lambda`, `alpha`: L2 and L1 regularization terms\n",
    "\n",
    "---\n",
    "\n",
    "### 12. **What is the concept of Feature Importance in Boosting?**\n",
    "\n",
    "Feature Importance shows **how much each feature contributes** to improving the model's performance. Boosting models measure it by:\n",
    "\n",
    "* **Frequency of usage** in splits\n",
    "* **Information gain** from splits\n",
    "* **Permutation importance** after training\n",
    "\n",
    "---\n",
    "\n",
    "### 13. **Why is CatBoost efficient for categorical data?**\n",
    "\n",
    "CatBoost handles categorical data efficiently by:\n",
    "\n",
    "* Automatically encoding categorical features without manual preprocessing\n",
    "* Using **ordered target statistics** to avoid overfitting\n",
    "* Optimizing performance and reducing leakage using smart encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca1657",
   "metadata": {},
   "source": [
    "---\n",
    "### PRACTICAL QUESTIONS:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30b538",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. **Train an AdaBoost Classifier on a sample dataset and print model accuracy**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Train an AdaBoost Regressor and evaluate performance using MAE**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model = AdaBoostRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "for name, importance in zip(data.feature_names, importances):\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Train a Gradient Boosting Regressor and evaluate using R-Squared Score**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting**\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "xgb_acc = accuracy_score(y_test, xgb.predict(X_test))\n",
    "gbc_acc = accuracy_score(y_test, gbc.predict(X_test))\n",
    "\n",
    "print(\"XGBoost Accuracy:\", xgb_acc)\n",
    "print(\"Gradient Boosting Accuracy:\", gbc_acc)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Train a CatBoost Classifier and evaluate using F1-Score**\n",
    "\n",
    "```python\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = CatBoostClassifier(verbose=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Train an XGBoost Regressor and evaluate using MSE**\n",
    "\n",
    "```python\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Train an AdaBoost Classifier and visualize feature importance**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.bar(range(X.shape[1]), model.feature_importances_)\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"AdaBoost Feature Importance\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Train a Gradient Boosting Regressor and plot learning curves**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_test, y_pred) for y_pred in model.staged_predict(X_test)]\n",
    "\n",
    "plt.plot(errors)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.title(\"Gradient Boosting Learning Curve\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Train an XGBoost Classifier and visualize feature importance**\n",
    "\n",
    "```python\n",
    "from xgboost import plot_importance\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plot_importance(model)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Train a CatBoost Classifier and plot the confusion matrix**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "model = CatBoostClassifier(verbose=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 12. **Train an AdaBoost Classifier with different numbers of estimators and compare accuracy**\n",
    "\n",
    "```python\n",
    "for n in [10, 50, 100, 200]:\n",
    "    model = AdaBoostClassifier(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"n_estimators={n}: Accuracy={acc:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 13. **Train a Gradient Boosting Classifier and visualize the ROC curve**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 14. **Train an XGBoost Regressor and tune the learning rate using GridSearchCV**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'learning_rate': [0.01, 0.1, 0.2, 0.3]}\n",
    "model = XGBRegressor()\n",
    "grid = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Learning Rate:\", grid.best_params_)\n",
    "print(\"Best Score (Neg MSE):\", grid.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 15. **Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting**\n",
    "\n",
    "```python\n",
    "# Assuming y is imbalanced\n",
    "model_bal = CatBoostClassifier(class_weights=[1, 5], verbose=0)\n",
    "model_bal.fit(X_train, y_train)\n",
    "y_pred = model_bal.predict(X_test)\n",
    "\n",
    "print(\"F1 Score with class weights:\", f1_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 16. **Train an AdaBoost Classifier and analyze the effect of different learning rates**\n",
    "\n",
    "```python\n",
    "for lr in [0.01, 0.1, 0.5, 1.0]:\n",
    "    model = AdaBoostClassifier(learning_rate=lr)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"Learning Rate={lr}: Accuracy={acc:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 17. **Train an XGBoost Classifier for multi-class classification and evaluate using log-loss**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model = XGBClassifier(objective='multi:softprob', num_class=10, use_label_encoder=False, eval_metric='mlogloss')\n",
    "model.fit(X_train, y_train)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "print(\"Log Loss:\", log_loss(y_test, y_proba))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646058b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
