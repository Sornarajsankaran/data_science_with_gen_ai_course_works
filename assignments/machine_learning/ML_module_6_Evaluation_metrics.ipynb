{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4573fe7d-6621-47d3-a22e-05ca77a5be66",
   "metadata": {},
   "source": [
    "___\n",
    "#  ML - MODULE 6 Evaluation metrics ASSIGNMENT\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb7ae3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 1: What does R-squared represent in a regression model </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> R-squared represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It is a measure of how well the regression model fits the observed data, with values ranging from 0 to 1, where a higher value indicates a better fit. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 2: What are the assumptions of linear regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> The assumptions of linear regression are: <br> 1. Linearity: The relationship between the independent and dependent variables is linear. <br> 2. Independence: Observations are independent of each other. <br> 3. Homoscedasticity: Constant variance of the errors across all levels of the independent variables. <br> 4. Normality: The residuals (errors) are normally distributed. <br> 5. No multicollinearity: Independent variables are not highly correlated with each other. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 3: What is the difference between R-squared and Adjusted R-squared </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> R-squared measures the proportion of variance explained by the model but can increase with the addition of irrelevant predictors. Adjusted R-squared adjusts for the number of predictors in the model, penalizing the addition of non-informative variables. Therefore, Adjusted R-squared provides a more accurate measure of model performance when multiple predictors are involved. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 4: Why do we use Mean Squared Error (MSE) </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Mean Squared Error (MSE) is used to measure the average squared difference between the observed actual outcomes and the predictions made by the model. It emphasizes larger errors due to the squaring, making it sensitive to outliers, and provides a clear metric for optimization during model training. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 5: What does an Adjusted R-squared value of 0.85 indicate </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> An Adjusted R-squared value of 0.85 indicates that approximately 85% of the variability in the dependent variable is explained by the independent variables in the model, after accounting for the number of predictors. It suggests a strong model fit. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 6: How do we check for normality of residuals in linear regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Normality of residuals can be checked using graphical methods such as Q-Q plots (quantile-quantile plots) or histograms of residuals, and statistical tests like the Shapiro-Wilk test or Kolmogorov-Smirnov test. These help determine whether residuals follow a normal distribution. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 7: What is multicollinearity, and how does it impact regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, leading to redundancy. It can inflate the variances of the coefficient estimates, making them unstable and unreliable, and complicates the interpretation of the effect of each predictor. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 8: What is Mean Absolute Error (MAE) </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Mean Absolute Error (MAE) is the average of the absolute differences between the predicted values and the actual values. It measures the average magnitude of errors without considering their direction, providing an easily interpretable metric of prediction accuracy. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 9: What are the benefits of using an ML pipeline </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> An ML pipeline offers benefits such as streamlined and reproducible workflows, easier management of data preprocessing, feature engineering, model training, and evaluation steps. It promotes modularity, reduces errors, enhances automation, and facilitates collaboration and scalability of machine learning projects. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 10: Why is RMSE considered more interpretable than MSE </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Root Mean Squared Error (RMSE) is considered more interpretable than MSE because it is in the same units as the target variable, making it easier to understand the average size of the prediction errors. In contrast, MSE is in squared units, which can be less intuitive to interpret. </div><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e78d047",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 11: What is pickling in Python, and how is it useful in ML </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Pickling in Python is the process of serializing an object into a byte stream so it can be saved to a file or transferred over a network. In machine learning, pickling is useful for saving trained models so they can be loaded later for inference or further training without retraining from scratch. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 12: What does a high R-squared value mean </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> A high R-squared value means that a large proportion of the variability in the dependent variable is explained by the independent variables in the model, indicating a good fit to the data. However, it does not guarantee that the model is appropriate or free from bias. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 13: What happens if linear regression assumptions are violated </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Violating linear regression assumptions can lead to biased or inefficient estimates, unreliable hypothesis tests, and poor predictive performance. For example, non-linearity can cause poor fit, heteroscedasticity affects the validity of confidence intervals, and multicollinearity inflates variance of coefficients. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 14: How can we address multicollinearity in regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Multicollinearity can be addressed by removing or combining correlated predictors, using dimensionality reduction techniques like Principal Component Analysis (PCA), or applying regularization methods such as Ridge or Lasso regression to stabilize coefficient estimates. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 15: How can feature selection improve model performance in regression analysis </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Feature selection improves model performance by removing irrelevant or redundant variables, which reduces overfitting, simplifies the model, enhances interpretability, and often leads to better generalization on unseen data. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 16: How is Adjusted R-squared calculated </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Adjusted R-squared is calculated using the formula: <br> Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)] <br> where n is the number of observations and p is the number of predictors. This adjustment penalizes the addition of non-informative predictors. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 17: Why is MSE sensitive to outliers </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> MSE is sensitive to outliers because it squares the errors, which disproportionately increases the impact of large errors compared to smaller ones, causing the metric to be heavily influenced by extreme values. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 18: What is the role of homoscedasticity in linear regression </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Homoscedasticity refers to the assumption that the variance of the residuals is constant across all levels of the independent variables. It is important because heteroscedasticity can lead to inefficient estimates and invalid standard errors, affecting hypothesis tests and confidence intervals. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 19: What is Root Mean Squared Error (RMSE) </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Root Mean Squared Error (RMSE) is the square root of the average squared differences between predicted and actual values. It provides a measure of the average magnitude of prediction errors in the same units as the target variable. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 20: Why is pickling considered risky </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Pickling is considered risky because loading pickled files can execute arbitrary code, posing a security vulnerability if the source of the pickle file is untrusted. Additionally, pickled objects may not be compatible across different Python versions or environments. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 21: What alternatives exist to pickling for saving ML models </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Alternatives to pickling include using frameworks' native save/load functions like joblib for scikit-learn models, TorchScript or ONNX for PyTorch models, TensorFlow SavedModel format, or PMML for model interoperability. These methods can be safer and more portable. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 22: What is heteroscedasticity, and why is it a problem </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Heteroscedasticity occurs when the variance of residuals varies across levels of the independent variables. It is a problem because it violates the assumption of constant error variance, leading to inefficient estimates and biased standard errors, which affect inference reliability. </div><hr> <div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 23: How can interaction terms enhance a regression model's predictive power </div> <hr> <div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\"> Interaction terms allow a regression model to capture the combined effect of two or more predictors on the dependent variable, which may be different from their individual effects. This can improve predictive power by modeling complex relationships in the data. </div><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a0e70",
   "metadata": {},
   "source": [
    "---\n",
    "## PRACTICAL QUESTIONS:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1450c6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **1. Visualize the distribution of residuals using Seaborn's diamonds dataset**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load diamonds dataset\n",
    "diamonds = sns.load_dataset(\"diamonds\").dropna()\n",
    "\n",
    "# Use numeric features only\n",
    "X = diamonds[['carat', 'depth', 'table']]\n",
    "y = diamonds['price']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot residuals\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title(\"Distribution of Residuals\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Calculate MSE, MAE, and RMSE**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_test and y_pred from previous code\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Check assumptions of linear regression**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Linearity check\n",
    "sns.scatterplot(x=y_pred, y=y_test)\n",
    "plt.title(\"Linearity Check: Predicted vs Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Residual plot (Homoscedasticity check)\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.title(\"Residuals Plot\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n",
    "\n",
    "# Multicollinearity check (correlation matrix)\n",
    "corr = diamonds[['carat', 'depth', 'table']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. ML pipeline with feature scaling and regression models**\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = {\n",
    "    \"Linear\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Random Forest\": RandomForestRegressor()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "    print(f\"{name} R² score: {scores.mean():.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Simple linear regression model**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=1)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Coefficient:\", model.coef_[0])\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"R² Score:\", model.score(X, y))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Analyze relationship between total bill and tip**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "tips = sns.load_dataset(\"tips\").dropna()\n",
    "\n",
    "X = tips[['total_bill']]\n",
    "y = tips['tip']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Prediction line\n",
    "sns.regplot(x=\"total_bill\", y=\"tip\", data=tips, line_kws={\"color\": \"red\"})\n",
    "plt.title(\"Total Bill vs Tip\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Fit linear regression on synthetic dataset**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + 4 + np.random.randn(100) * 2\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "plt.plot(X, model.predict(X), color='red', label=\"Regression Line\")\n",
    "plt.title(\"Linear Regression on Synthetic Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Pickle a trained model**\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "# Save model to file\n",
    "with open(\"linear_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved as linear_model.pkl\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Polynomial regression (degree 2)**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Generate data\n",
    "X = np.random.rand(100, 1) * 6 - 3\n",
    "y = 0.5 * X.squeeze() ** 2 + X.squeeze() + 2 + np.random.randn(100)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Plot\n",
    "X_range = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y_range = model.predict(poly.transform(X_range))\n",
    "\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "plt.plot(X_range, y_range, color='red', label=\"Polynomial Fit\")\n",
    "plt.title(\"Polynomial Regression (Degree 2)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Generate and fit linear regression on synthetic data**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.random.rand(100, 1) * 5\n",
    "y = 2 * X.squeeze() + 1 + np.random.randn(100)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Coefficient:\", model.coef_[0])\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5d741",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **14. VIF for multicollinearity**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.datasets import load_boston\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load dataset\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "# Add intercept for statsmodels\n",
    "X[\"Intercept\"] = 1\n",
    "\n",
    "# Calculate VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **15. Polynomial regression (degree 4)**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Generate data\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = 1 * X.squeeze()**4 - 2 * X.squeeze()**3 + 0.5 * X.squeeze()**2 + X.squeeze() + np.random.randn(100) * 10\n",
    "\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Plot\n",
    "y_pred = model.predict(X_poly)\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "plt.plot(X, y_pred, color=\"red\", label=\"Degree 4 Fit\")\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Regression (Degree 4)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **16. Pipeline with standardization and multiple linear regression**\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=5, random_state=1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "print(\"R-squared score:\", pipeline.score(X, y))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **17. Polynomial regression (degree 3)**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(-2, 2, 100).reshape(-1, 1)\n",
    "y = X.squeeze()**3 - 2*X.squeeze()**2 + 3*X.squeeze() + 5 + np.random.randn(100)\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "y_pred = model.predict(X_poly)\n",
    "\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "plt.plot(X, y_pred, color='red', label=\"Degree 3 Fit\")\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Regression (Degree 3)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **18. Multiple linear regression with 5 features**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"R-squared Score:\", model.score(X, y))\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **19. Simple linear regression with visualization**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2.5 * X.squeeze() + 3 + np.random.randn(100)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "plt.plot(X, model.predict(X), color='red', label=\"Regression Line\")\n",
    "plt.title(\"Simple Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **20. Multiple linear regression with 3 features**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=8)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"R-squared Score:\", model.score(X, y))\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **21. Serialize and deserialize using joblib**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "import joblib\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=5)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, 'model_joblib.pkl')\n",
    "print(\"Model saved using joblib\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = joblib.load('model_joblib.pkl')\n",
    "print(\"Loaded model coefficient:\", loaded_model.coef_[0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **22. Linear regression with categorical features (one-hot)**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "tips = sns.load_dataset(\"tips\").dropna()\n",
    "\n",
    "# One-hot encode 'sex', 'smoker', 'day', 'time'\n",
    "X = pd.get_dummies(tips.drop(\"tip\", axis=1), drop_first=True)\n",
    "y = tips[\"tip\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"R-squared Score:\", model.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **23. Compare Ridge vs Linear Regression**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10)\n",
    "\n",
    "lin_model = LinearRegression()\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "lin_model.fit(X, y)\n",
    "ridge_model.fit(X, y)\n",
    "\n",
    "print(\"Linear Regression Coefficients:\", lin_model.coef_)\n",
    "print(\"Ridge Regression Coefficients:\", ridge_model.coef_)\n",
    "print(\"Linear R²:\", lin_model.score(X, y))\n",
    "print(\"Ridge R²:\", ridge_model.score(X, y))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **24. Cross-validation for linear regression**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=4, noise=5)\n",
    "model = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "print(\"Cross-Validation R² Scores:\", scores)\n",
    "print(\"Average R² Score:\", scores.mean())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **25. Compare polynomial regression of different degrees**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = 2 * X.squeeze()**3 - X.squeeze()**2 + X.squeeze() + np.random.randn(100) * 5\n",
    "\n",
    "for degree in range(1, 6):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    r2 = model.score(X_poly, y)\n",
    "    print(f\"Degree {degree} R² Score: {r2:.4f}\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12107b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93f26f86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41da2a15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8b4893f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f564a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c0e73f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fffc9e54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9613667",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "369f245a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e410918",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
