{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcaf317b",
   "metadata": {},
   "source": [
    "___\n",
    "#  ML - MODULE 9 SVM & Naive bayes ASSIGNMENT\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0432c56e",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 1: What is a Support Vector Machine (SVM) </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates data points of different classes in a high-dimensional space. The main goal of SVM is to maximize the margin between the nearest data points (support vectors) of each class and the decision boundary.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 2: What is the difference between Hard Margin and Soft Margin SVM </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Hard Margin SVM assumes that the data is linearly separable and finds a hyperplane that perfectly separates the classes without any misclassification.  \n",
    "Soft Margin SVM allows for some misclassifications to handle noisy or non-linearly separable data. It introduces a regularization parameter (C) to balance margin maximization and classification error.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 3: What is the mathematical intuition behind SVM </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "SVM tries to solve the optimization problem of maximizing the margin between two classes. Mathematically, it minimizes:  \n",
    "<pre style=\"font-family: Verdana;\">  \n",
    "    1/2 ||w||²  \n",
    "</pre>  \n",
    "subject to the constraint:  \n",
    "<pre style=\"font-family: Verdana;\">  \n",
    "    yᵢ(w · xᵢ + b) ≥ 1  \n",
    "</pre>  \n",
    "where w is the weight vector, b is the bias, xᵢ is a data point, and yᵢ is its label. The margin is 2/||w||, and maximizing it means minimizing ||w||².  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 4: What is the role of Lagrange Multipliers in SVM </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Lagrange Multipliers are used to solve the constrained optimization problem in SVM. They help in formulating the dual problem, which is often easier to solve and allows the introduction of kernels. The solution gives the weight vector as a combination of support vectors and their corresponding multipliers.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 5: What are Support Vectors in SVM </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Support Vectors are the data points that lie closest to the decision boundary (hyperplane). They are the critical elements that define the margin and influence the position of the separating hyperplane. Removing them would change the model.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 6: What is a Support Vector Classifier (SVC) </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Support Vector Classifier (SVC) is the implementation of SVM used for classification problems. It uses support vectors to determine the decision boundary that best separates different classes, possibly in a transformed feature space via kernels.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 7: What is a Support Vector Regressor (SVR) </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Support Vector Regressor (SVR) is the regression version of SVM. It tries to find a function that approximates the target values within a margin of tolerance (epsilon) while minimizing model complexity. Points outside the epsilon margin become support vectors.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 8: What is the Kernel Trick in SVM </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "The Kernel Trick allows SVM to operate in a high-dimensional feature space without explicitly computing the coordinates. Instead, it computes the inner product between two data points in the transformed space using a kernel function, enabling non-linear classification.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 9: Compare Linear Kernel, Polynomial Kernel, and RBF Kernel </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "- Linear Kernel: Suitable for linearly separable data. Fast and less complex.  \n",
    "- Polynomial Kernel: Maps input features into polynomial space. Useful for problems with interaction features.  \n",
    "- RBF (Radial Basis Function) Kernel: Maps input data to infinite-dimensional space. Effective for complex, non-linear boundaries.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 10: What is the effect of the C parameter in SVM </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "The C parameter controls the trade-off between achieving a low error on the training data and maintaining a large margin.  \n",
    "- High C: Focuses on minimizing training error, may lead to overfitting.  \n",
    "- Low C: Allows more misclassifications, resulting in a wider margin and better generalization.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 11: What is the role of the Gamma parameter in RBF Kernel SVM </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Gamma defines how far the influence of a single training example reaches.  \n",
    "- High Gamma: Each point has a short reach; the model captures fine patterns but may overfit.  \n",
    "- Low Gamma: Points have broader influence; the model becomes smoother but may underfit.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 12: What is the Naïve Bayes classifier, and why is it called \"Naïve\" </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It is called \"naïve\" because it assumes that all features are independent given the class label, which is a strong and often unrealistic assumption. Despite this, it performs well in many practical scenarios.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 13: What is Bayes’ Theorem </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Bayes’ Theorem describes the probability of an event based on prior knowledge of related events.  \n",
    "<pre style=\"font-family: Verdana;\">  \n",
    "    P(A|B) = [P(B|A) * P(A)] / P(B)  \n",
    "</pre>  \n",
    "In classification, A is the class, and B is the observed features. It calculates the posterior probability of a class given the input features.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 14: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "- Gaussian Naïve Bayes: Assumes features follow a normal distribution. Suitable for continuous data.  \n",
    "- Multinomial Naïve Bayes: Assumes features are count-based. Best for document classification using term frequencies.  \n",
    "- Bernoulli Naïve Bayes: Works with binary features (present/absent). Useful for text data where word presence matters.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 15: When should you use Gaussian Naïve Bayes over other variants </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Gaussian Naïve Bayes is suitable when the features are continuous and follow a bell-curve (normal distribution). It is commonly used in sensor data, medical data, or datasets where numeric attributes dominate.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 16: What are the key assumptions made by Naïve Bayes </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "- Feature independence: Each feature contributes independently to the probability of a class.  \n",
    "- Equal importance: All features are considered equally relevant.  \n",
    "These assumptions simplify computation but may not hold in real-world data.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 17: What are the advantages and disadvantages of Naïve Bayes </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Advantages:  \n",
    "- Simple, fast, and scalable.  \n",
    "- Works well with high-dimensional data.  \n",
    "- Performs well even with unrealistic assumptions.  \n",
    "Disadvantages:  \n",
    "- Assumes feature independence, which may reduce accuracy.  \n",
    "- Struggles with feature correlation and numeric data with non-Gaussian distribution.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 18: Why is Naïve Bayes a good choice for text classification </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Naïve Bayes is effective for text classification because text data often has high dimensionality and sparse features. The independence assumption works well for word counts or word presence. It is fast, requires less training data, and performs competitively with more complex models.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 19: Compare SVM and Naïve Bayes for classification tasks </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "- SVM: More accurate on complex data; handles non-linear decision boundaries via kernels; slower training; sensitive to parameter tuning.  \n",
    "- Naïve Bayes: Fast, simple, scalable; performs well on high-dimensional data like text; may underperform if independence assumption is violated.  \n",
    "</div>  \n",
    "<hr>  \n",
    "\n",
    "<div style=\"font-family: Verdana; font-size: 20px; font-weight: bold; color: black;\"> Question 20: How does Laplace Smoothing help in Naïve Bayes </div>  \n",
    "<hr>  \n",
    "<div style=\"font-family: Verdana; font-size: 18px; line-height: 1.6;\">  \n",
    "Laplace Smoothing adds a small constant (usually 1) to each count to avoid zero probability for unseen features. This helps in improving model robustness when encountering words or features not present in the training data.  \n",
    "</div>  \n",
    "<hr>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748366b",
   "metadata": {},
   "source": [
    "---\n",
    "### PRACTICAL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037654f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  1. SVM Classifier on Iris dataset\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  2. SVM with Linear and RBF Kernels on Wine dataset\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "linear_svm = SVC(kernel='linear')\n",
    "rbf_svm = SVC(kernel='rbf')\n",
    "\n",
    "linear_svm.fit(X_train, y_train)\n",
    "rbf_svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Linear Kernel Accuracy:\", accuracy_score(y_test, linear_svm.predict(X_test)))\n",
    "print(\"RBF Kernel Accuracy:\", accuracy_score(y_test, rbf_svm.predict(X_test)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  3. SVM Regressor (SVR) on housing dataset\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVR()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  4. SVM with Polynomial Kernel and Decision Boundary\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "model = SVC(kernel='poly', degree=3)\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Plotting decision boundary\n",
    "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.5, cmap=plt.cm.coolwarm)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, edgecolors='k')\n",
    "plt.title(\"SVM with Polynomial Kernel\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  5. Gaussian Naïve Bayes on Breast Cancer dataset\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  6. Multinomial Naïve Bayes on 20 Newsgroups Text Dataset\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "model.fit(data.data, data.target)\n",
    "\n",
    "y_pred = model.predict(test_data.data)\n",
    "print(\"Accuracy:\", accuracy_score(test_data.target, y_pred))\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcae807",
   "metadata": {},
   "source": [
    "Got it! Here are the Python programs for each task you requested, one by one:\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Feature Selection before Naïve Bayes & compare results\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Naive Bayes without feature selection\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_full = nb.predict(X_test)\n",
    "acc_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "# Feature selection: select top 10 features using chi-squared test\n",
    "selector = SelectKBest(score_func=chi2, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "nb_selected = GaussianNB()\n",
    "nb_selected.fit(X_train_selected, y_train)\n",
    "y_pred_selected = nb_selected.predict(X_test_selected)\n",
    "acc_selected = accuracy_score(y_test, y_pred_selected)\n",
    "\n",
    "print(f\"Accuracy without feature selection: {acc_full:.4f}\")\n",
    "print(f\"Accuracy with top 10 features: {acc_selected:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. SVM Classifier using OvR and OvO on Wine dataset\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# OvR\n",
    "ovr_clf = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "ovr_clf.fit(X_train, y_train)\n",
    "y_pred_ovr = ovr_clf.predict(X_test)\n",
    "\n",
    "# OvO\n",
    "ovo_clf = OneVsOneClassifier(SVC(kernel='linear'))\n",
    "ovo_clf.fit(X_train, y_train)\n",
    "y_pred_ovo = ovo_clf.predict(X_test)\n",
    "\n",
    "print(\"OvR Accuracy:\", accuracy_score(y_test, y_pred_ovr))\n",
    "print(\"OvO Accuracy:\", accuracy_score(y_test, y_pred_ovo))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. SVM Classifier with Linear, Polynomial, RBF kernels on Breast Cancer dataset\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "for kernel in kernels:\n",
    "    model = SVC(kernel=kernel, degree=3 if kernel == 'poly' else 3)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{kernel.capitalize()} Kernel Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. SVM Classifier with Stratified K-Fold Cross-Validation average accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(svm, X, y, cv=skf)\n",
    "\n",
    "print(f\"Average Accuracy with Stratified K-Fold CV: {np.mean(scores):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Naïve Bayes with different prior probabilities and compare\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "priors_list = [None, [0.5, 0.5], [0.7, 0.3]]\n",
    "for priors in priors_list:\n",
    "    nb = GaussianNB(priors=priors)\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred = nb.predict(X_test)\n",
    "    print(f\"Priors={priors}: Accuracy={accuracy_score(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Recursive Feature Elimination (RFE) before SVM Classifier and compare accuracy\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Without RFE\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "acc_full = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "# With RFE: select top 5 features\n",
    "selector = RFE(estimator=SVC(kernel='linear'), n_features_to_select=5)\n",
    "X_train_rfe = selector.fit_transform(X_train, y_train)\n",
    "X_test_rfe = selector.transform(X_test)\n",
    "\n",
    "model_rfe = SVC(kernel='linear')\n",
    "model_rfe.fit(X_train_rfe, y_train)\n",
    "acc_rfe = accuracy_score(y_test, model_rfe.predict(X_test_rfe))\n",
    "\n",
    "print(f\"Accuracy without RFE: {acc_full:.4f}\")\n",
    "print(f\"Accuracy with RFE (top 5 features): {acc_rfe:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 13. SVM Classifier with Precision, Recall, F1-Score evaluation\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Naïve Bayes Classifier evaluated using Log Loss (Cross-Entropy Loss)\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "print(f\"Log Loss: {log_loss(y_test, y_proba):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 15. SVM Classifier Confusion Matrix visualization using seaborn\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 16. SVM Regressor evaluated with Mean Absolute Error (MAE)\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVR()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 17. Naïve Bayes Classifier evaluated using ROC-AUC score\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 18. SVM Classifier visualizing Precision-Recall Curve\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVC(probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_scores = model.predict_proba(X_test)\\[:, 1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "avg_precision = average_precision_score(y_test, y_scores)\n",
    "plt.step(recall, precision, where='post')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(f'Precision-Recall curve: AP={avg_precision:.4f}')\n",
    "plt.show()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933dded9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
